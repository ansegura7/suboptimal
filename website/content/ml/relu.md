Title: Rectified Linear Unit (ReLU)
TitleShort: Rectified Linear Unit
Date: 2021-05-08
Author: Vladimir Haltakov
AuthorLink: https://twitter.com/haltakov
Category: Machine Learning Terms
Slug: relu
Summary: A simple function of `x` returning 0 if `x < 0` or `x` otherwise.

A simple function of `x` returning 0 if `x < 0` or `x` otherwise.

ReLU is a popular activation function in deep neural networks because it is fast to compute and helps avoid the vanishing gradients problem.

<img class="w-full md:w-1/2 lg:w-3/5 mx-auto my-4" src="{{ SITEURL }}/images/relu.jpg" alt="Plot of the ReLU activation function">
